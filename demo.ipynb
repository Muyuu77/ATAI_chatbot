{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import fasttext\n",
    "import rdflib\n",
    "import re\n",
    "import editdistance\n",
    "import pickle\n",
    "\n",
    "# import en_core_web_trf\n",
    "# import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KG_handler:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        ### Notices \n",
    "            - all2lbl has more \"P\" item than ent2id dose (ent2lbl:298, ent2id:47)\n",
    "            - all2lbl contain all entities and relations, while ent2id lacks some relations\n",
    "            - labels of entity are not unique, but entity URIs are unique\n",
    "            - labels of relations are unique (at least unique till now)     \n",
    "            - to obtain query, we need movie name (entity label), relation label and relation URI\n",
    "        '''\n",
    "        \n",
    "        # language model\n",
    "        print('step 3')\n",
    "        self.LM = fasttext.load_model('utils/cc.en.300.bin')\n",
    "        try:\n",
    "            # self.spacy_model = en_core_web_trf.load()\n",
    "            self.spacy_model = spacy.load('en_core_web_trf')\n",
    "        except OSError:\n",
    "            print('Can\\'t find model, please run command \"python -m spacy download en_core_web_trf\" to download it and restart the program')\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # RDF config\n",
    "        print('step 1')\n",
    "        self.graph = rdflib.Graph().parse('utils/14_graph.nt', format='turtle')\n",
    "        self.WD = rdflib.Namespace('http://www.wikidata.org/entity/')\n",
    "        self.WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
    "        self.DDIS = rdflib.Namespace('http://ddis.ch/atai/')\n",
    "        self.RDFS = rdflib.namespace.RDFS\n",
    "        self.SCHEMA = rdflib.Namespace('http://schema.org/')      \n",
    "        \n",
    "          \n",
    "        # label relevant\n",
    "        print('step 2')\n",
    "        self.all2lbl = {ent: str(lbl) for ent, lbl in self.graph.subject_objects(self.RDFS.label)}\n",
    "        self.rel2lbl = {k:v for k, v in self.all2lbl.items() if self._is_rel(k)}\n",
    "        self.lbl2rel = {lbl: ent for ent, lbl in self.rel2lbl.items()}\n",
    "        self.rel_lbl_set = set([v for v in self.rel2lbl.values()])\n",
    "        self.ent2lbl = {k:v for k, v in self.all2lbl.items() if self._is_ent(k)}\n",
    "        self.lbl2ent = {lbl: ent for ent, lbl in self.ent2lbl.items()}  # not unique\n",
    "        self.ent_lbl_set = set([v for v in self.ent2lbl.values()])\n",
    "        # with open('utils/ent_lbl2vec.pkl', 'rb') as f:\n",
    "        #     self.ent_lbl2vec = pickle.load(f)\n",
    "        # with open('utils/rel_lbl2vec.pkl', 'rb') as f:\n",
    "        #     self.rel_lbl2vec = pickle.load(f)\n",
    "        self.ent_lbl2vec = {k:self.LM.get_word_vector(k) for k in self.ent_lbl_set}\n",
    "        self.rel_lbl2vec = {k:self.LM.get_word_vector(k) for k in self.rel_lbl_set}\n",
    "        # self.all_lbl_set\n",
    "        \n",
    "        # embedding relevant\n",
    "        # with open('utils/relation_ids.del', 'r') as ifile:\n",
    "        #     self.rel2id = {rdflib.term.URIRef(rel): int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\n",
    "        # self.id2rel = {v: k for k, v in self.rel2id.items()}\n",
    "        # with open('utils/entity_ids.del', 'r') as ifile:\n",
    "        #     self.ent2id = {rdflib.term.URIRef(ent): int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\n",
    "        # self.id2ent = {v: k for k, v in self.ent2id.items()}\n",
    "        # self.ent_emb = np.load('utils/entity_embeds.npy')\n",
    "        # self.rel_emb = np.load('utils/relation_embeds.npy')\n",
    "        \n",
    "        self.synonyms_dict = {\n",
    "            'cast member' :['actor', 'actress', 'cast'],\n",
    "            'genre': ['type', 'kind'],\n",
    "            'publication date': ['release', 'date', 'airdate', 'publication', 'launch', 'broadcast','released','launched'],\n",
    "            'executive producer': ['showrunner'],\n",
    "            'screenwriter': ['scriptwriter', 'screenplay', 'teleplay', 'writer', 'script', 'scenarist', 'story'],\n",
    "            'director of photography': ['cinematographer', 'DOP', 'dop'],\n",
    "            'film editor': ['editor'],\n",
    "            'production designer': ['designer'],\n",
    "            'box office': ['box', 'office', 'funding'],\n",
    "            'cost': ['budget', 'cost'],\n",
    "            'nominated for': ['nomination', 'award', 'finalist', 'shortlist', 'selection'],\n",
    "            'costume designer': ['costume'],\n",
    "            'official website': ['website', 'site'],\n",
    "            'filming location': ['flocation'],\n",
    "            'narrative website': ['nlocation'],\n",
    "            'production company': ['company'],\n",
    "            'country of origin': ['origin', 'country'],\n",
    "            'â€“' : ['-']\n",
    "        }\n",
    "        self.replacement_dict = {k:v for k,  v_list in self.synonyms_dict.items() for v in v_list }\n",
    "        \n",
    "    def _ruler_based(self, query:str):\n",
    "        '''\n",
    "        Param:\n",
    "            query: natrual language query from the user\n",
    "        Return:\n",
    "            {'ent_lbl': None | str, 'rel_lbl': None | str, 'rel_postfix': None | str}:\n",
    "            diction of extraction results. if not found than set as None\n",
    "        '''\n",
    "        \n",
    "        res = {'ent_lbl': None,\n",
    "               'rel': None,\n",
    "               'rel_lbl': None,\n",
    "               'rel_postfix': None}\n",
    "        \n",
    "        # pre-proecssing\n",
    "        tokens = self._replace(query)\n",
    "        \n",
    "        # get all word combination that match existed entities\n",
    "        word_seq = [' '.join(tokens[i:j+1]) for i in range(len(tokens)) for j in range(i, len(tokens))]\n",
    "        matched_seq = [seq for seq in word_seq if seq in (self.ent_lbl_set | self.rel_lbl_set)]      # all exited entities that appear in the sentence\n",
    "        \n",
    "        # extraction \n",
    "        ent_candidates = []\n",
    "        for seq in matched_seq:\n",
    "            if seq in self.ent_lbl_set:\n",
    "                ent_candidates.append(seq)\n",
    "            # detected relation\n",
    "            if seq in self.rel_lbl_set:\n",
    "                # which means there are two possible word for relations\n",
    "                if res['rel_lbl'] != None:\n",
    "                    print(\"WARNIND: multiple possible relations detected...\")\n",
    "                res['rel'] = seq\n",
    "                res['rel_lbl'] = self.lbl2rel[seq]\n",
    "                res['rel_postfix'] = self._get_rel_label(res['rel_lbl'])\n",
    "        # process possible entities\n",
    "        ent_candidates = sorted(ent_candidates, key = lambda x:len(x[0]), reverse=True)\n",
    "        res['ent_lbl'] = ent_candidates[0]\n",
    "    \n",
    "        return res\n",
    "    \n",
    "    def _similarity_based(self, query, top_k_rel=10, top_k_ent=1):\n",
    "        '''\n",
    "        similar to _rule_based(), but return top k similar results in list form, which is:\n",
    "        Return:\n",
    "            {'ent_lbl': list(str), 'rel_lbl': list(str), 'rel_postfix': list(str)}:\n",
    "        '''\n",
    "        \n",
    "        res = {'ent_lbl': None,\n",
    "               'rel': None,\n",
    "               'rel_lbl': None,\n",
    "               'rel_postfix': None}\n",
    "        \n",
    "        # pre-processing\n",
    "        tokens = self._replace(query).join()\n",
    "        \n",
    "        # token process\n",
    "        doc = self.spacy_model(tokens)\n",
    "        \n",
    "        \n",
    "        ent = []    # entity\n",
    "        rel = []    # relation\n",
    "        for token in doc:\n",
    "            # print(token)\n",
    "            if (token.ent_iob_ != \"O\"):\n",
    "                ent.append(token.lemma_)\n",
    "            elif (token.pos_=='NOUN') | (token.pos_==\"VERB\"):\n",
    "                rel.append(token.lemma_)\n",
    "                \n",
    "                \n",
    "        # find the closest relation information\n",
    "        rel_wv = np.array([i for i in self.rel_lbl2vec.values()])\n",
    "        rel_lbl = [i for i in self.rel_lbl2vec.keys()]\n",
    "        rel = \" \".join(rel)\n",
    "        wv = self.LM.get_word_vector(rel).reshape((1,-1))\n",
    "        dist = pairwise_distances(wv, rel_wv).flatten()\n",
    "        closest_rel_idx = dist.argsort()[:top_k_rel]\n",
    "        closest_rel_lbl = [rel_lbl[i] for i in closest_rel_idx]\n",
    "        closest_rel_uri = [self.lbl2rel[i] for i in closest_rel_lbl]\n",
    "        \n",
    "        \n",
    "        # find the closest ent information\n",
    "        extracted_ent = \" \".join(ent)\n",
    "        ent_wv = np.array([i for i in self.ent_lbl2vec.values()])\n",
    "        ent_lbl = [i for i in self.ent_lbl2vec.keys()]\n",
    "        wv = self.LM.get_word_vector(extracted_ent).reshape((1,-1))\n",
    "        dist = pairwise_distances(wv, ent_wv).flatten()\n",
    "        closest_ent_idx = dist.argsort()[:top_k_ent]\n",
    "        closest_ent_lbl = [ent_lbl[i] for i in closest_ent_idx]\n",
    "\n",
    "        res['rel'] = rel\n",
    "        res['ent_lbl'] = closest_ent_lbl[0]\n",
    "        res['rel_lbl'] = closest_rel_lbl\n",
    "        res['rel_postfix'] = [self._get_rel_label(uri) for uri in closest_rel_uri]\n",
    "\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    def _replace(self, sent:str) -> list :\n",
    "        '''\n",
    "        replace words in sentence if they have relevant replacement in the dictionary\n",
    "        '''\n",
    "        \n",
    "        # remove all possible punctuation in the end of sentence\n",
    "        cleaned_sent = sent.rstrip(string.punctuation + ' ')\n",
    "        tokens = cleaned_sent.split()\n",
    "        tokens = [ self.replacement_dict[token] if token in self.replacement_dict.keys() else token for token in tokens ]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def get_query_res(self, user_input):\n",
    "        \n",
    "        '''\n",
    "        Return:\n",
    "            2D-list (n_queries, n_cols_per_query)\n",
    "        '''\n",
    "        \n",
    "        ruler_attemp = self._ruler_based(user_input)\n",
    "        # print(ruler_attemp)\n",
    "\n",
    "        # calling backup extraction strategy\n",
    "        if None in ruler_attemp.values():\n",
    "            similarity_attemp = self._similarity_based(user_input)\n",
    "            for k,v in ruler_attemp.items():\n",
    "                if v == None:\n",
    "                    ruler_attemp[k] = similarity_attemp[k]    \n",
    "        similarity_attemp = self._similarity_based(user_input)\n",
    "\n",
    "\n",
    "        # print(similarity_attemp)\n",
    "\n",
    "        # grab result from graph\n",
    "        for i in range(len(ruler_attemp['rel_postfix'])):\n",
    "            movie_name = ruler_attemp['ent_lbl']\n",
    "            target_label = ruler_attemp['rel_postfix'][i]\n",
    "            target_name = ruler_attemp['rel_lbl'][i]\n",
    "            if \"date\" in target_name:\n",
    "                query = f'''PREFIX ddis: <http://ddis.ch/atai/>\n",
    "\n",
    "                PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "\n",
    "                PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "\n",
    "                PREFIX schema: <http://schema.org/>\n",
    "\n",
    "                SELECT ?date WHERE {{\n",
    "                    ?movie rdfs:label \"{movie_name}\"@en.\n",
    "\n",
    "                    ?movie wdt:{target_label} ?date\n",
    "\n",
    "                }} LIMIT 1'''\n",
    "            else:\n",
    "                query = f'''PREFIX ddis: <http://ddis.ch/atai/>\n",
    "\n",
    "                PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "\n",
    "                PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "\n",
    "                PREFIX schema: <http://schema.org/>\n",
    "\n",
    "                SELECT ?lbl WHERE {{\n",
    "                    ?sub rdfs:label \"{movie_name}\"@en.\n",
    "\n",
    "                    ?sub wdt:{target_label} ?obj.\n",
    "\n",
    "                    ?obj rdfs:label ?lbl.\n",
    "\n",
    "                }} LIMIT 1'''\n",
    "            query  = query.strip()\n",
    "            # print(query)\n",
    "            \n",
    "            res = []\n",
    "            for row in KG_handler.graph.query(query):\n",
    "                res.append([str(i) for i in row]) \n",
    "            # print(res)\n",
    "            if len(res) != 0:\n",
    "                break\n",
    "            \n",
    "        return res    \n",
    "    \n",
    "    \n",
    "    def _get_rel_label(self, URI):\n",
    "        return str(URI).split('/')[-1]\n",
    "    \n",
    "    def _is_rel(self, URI):\n",
    "        label = self._get_rel_label(URI)\n",
    "        return label[0] == 'P'\n",
    "    \n",
    "    def _is_ent(self, URI):\n",
    "        label = self._get_rel_label(URI)\n",
    "        return label[0] == 'Q'   \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3\n",
      "step 1\n",
      "step 2\n"
     ]
    }
   ],
   "source": [
    "KG_handler = KG_handler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Star Wars Episode IX: The Rise of Skywalker\n",
      "Star Wars Episode III: Revenge of the Sith\n",
      "Star Wars Episode I: The Phantom Menace\n",
      "Star Wars Episode VI: Return of the Jedi\n",
      "Star Wars Episode VII: The Force Awakens\n",
      "Star Wars Episode I: Battle for Naboo\n",
      "Star Wars Episode II: Attack of the Clones\n",
      "Star Wars Episode V: The Empire Strikes Back\n",
      "Star Wars: Episode II â€“ Attack of the Clones\n",
      "Star Wars: Episode III â€“ Revenge of the Sith (soundtrack)\n",
      "Star Wars: Episode I â€“ The Phantom Menace\n",
      "Star Wars: Episode I â€“ The Phantom Menace (soundtrack)\n",
      "Star Wars: Episode VI â€“ Return of the Jedi\n",
      "Star Wars: Episode III â€“ Revenge of the Sith\n",
      "Star Wars: Episode VIII â€“ The Last Jedi\n",
      "Star Wars: Episode IV â€“ A New Hope\n",
      "Star Wars: Episode V â€“ The Empire Strikes Back\n"
     ]
    }
   ],
   "source": [
    "# for i in KG_handler.ent_lbl_set:\n",
    "#     if 'Star Wars Episode' in i:\n",
    "#         print(i)\n",
    "# for i in KG_handler.ent_lbl_set:\n",
    "#     if 'Star Wars: Episode' in i:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _similarity_based(model, query, top_k_rel=10, top_k_ent=10):\n",
    "    '''\n",
    "    similar to _rule_based(), but return top k similar results in list form, which is:\n",
    "    Return:\n",
    "        {'ent_lbl': list(str), 'rel_lbl': list(str), 'rel_postfix': list(str)}:\n",
    "    '''\n",
    "\n",
    "\n",
    "    res = {'ent_lbl': None,\n",
    "            'rel': None,\n",
    "            'rel_lbl': None,\n",
    "            'rel_postfix': None}\n",
    "    replacement_dict = {v:k for k,  v_list in model.synonyms_dict.items() for v in v_list }\n",
    "    replacement_dict['-'] = 'â€“'\n",
    "\n",
    "    \n",
    "    # remove all possible punctuation in the end of sentence\n",
    "    cleaned_sentence = query.rstrip(string.punctuation+' ')\n",
    "    # print(cleaned_sentence)\n",
    "    tokens = cleaned_sentence.split()\n",
    "    tokens = [ replacement_dict[token] if token in replacement_dict.keys() else token for token in tokens ]\n",
    "    doc = model.spacy_model(' '.join(tokens))\n",
    "    \n",
    "    ent = []    # entity\n",
    "    rel = []    # relation\n",
    "\n",
    "\n",
    "    for token in doc:\n",
    "        # print(token, token.ent_iob_)\n",
    "        if (token.ent_iob_ != \"O\"):\n",
    "            ent.append(token.lemma_)\n",
    "        elif (token.pos_=='NOUN') | (token.pos_==\"VERB\"):\n",
    "            rel.append(token.lemma_)\n",
    "            \n",
    "    # find the closest relation information\n",
    "    rel_wv = np.array([i for i in model.rel_lbl2vec.values()])\n",
    "    rel_lbl = [i for i in model.rel_lbl2vec.keys()]\n",
    "    rel = \" \".join(rel)\n",
    "    # rel = 'release'\n",
    "    wv = model.LM.get_word_vector(rel).reshape((1,-1))\n",
    "    dist = pairwise_distances(wv, rel_wv).flatten()\n",
    "    closest_rel_idx = dist.argsort()[:top_k_rel]\n",
    "    closest_rel_lbl = [rel_lbl[i] for i in closest_rel_idx]\n",
    "    closest_rel_uri = [model.lbl2rel[i] for i in closest_rel_lbl]\n",
    "    \n",
    "    # find the closest ent information\n",
    "    extracted_ent = \" \".join(ent)\n",
    "    # print(extracted_ent)\n",
    "    ent_wv = np.array([i for i in model.ent_lbl2vec.values()])\n",
    "    ent_lbl = [i for i in model.ent_lbl2vec.keys()]\n",
    "    wv = model.LM.get_word_vector(extracted_ent).reshape((1,-1))\n",
    "    dist = pairwise_distances(wv, ent_wv).flatten()\n",
    "    closest_ent_idx = dist.argsort()[:top_k_ent]\n",
    "    # print(dist[dist.argsort()[:top_k_ent]])\n",
    "    closest_ent_lbl = [ent_lbl[i] for i in closest_ent_idx]\n",
    "    print(closest_ent_lbl)\n",
    "    # print(closest_ent_lbl)\n",
    "\n",
    "    res['rel'] = rel\n",
    "    res['ent_lbl'] = closest_ent_lbl[0]\n",
    "    res['rel_lbl'] = closest_rel_lbl\n",
    "    res['rel_postfix'] = [model._get_rel_label(uri) for uri in closest_rel_uri]\n",
    "\n",
    "    return res\n",
    "\n",
    "def _ruler_based(model, query:str):\n",
    "    '''\n",
    "    Param:\n",
    "        query: natrual language query from the user\n",
    "    Return:\n",
    "        {'ent_lbl': None | str, 'rel_lbl': None | str, 'rel_postfix': None | str}:\n",
    "        diction of extraction results. if not found than set as None\n",
    "    '''\n",
    "    \n",
    "    res = {'ent_lbl': None,\n",
    "            'rel': None,\n",
    "            'rel_lbl': None,\n",
    "            'rel_postfix': None}\n",
    "    replacement_dict = {v:k for k,  v_list in model.synonyms_dict.items() for v in v_list }\n",
    "    replacement_dict['-'] = 'â€“'\n",
    "\n",
    "    \n",
    "    # remove all possible punctuation in the end of sentence\n",
    "    cleaned_sentence = query.rstrip(string.punctuation+' ')\n",
    "    # print(cleaned_sentence)\n",
    "    tokens = cleaned_sentence.split()\n",
    "    \n",
    "    # matching possible existing entities and relations from sentence\n",
    "    tokens = [ replacement_dict[token] if token in replacement_dict.keys() else token for token in tokens ]\n",
    "    word_seq = [' '.join(tokens[i:j+1]) for i in range(len(tokens)) for j in range(i, len(tokens))]\n",
    "    matched_seq = [seq for seq in word_seq if seq in (model.ent_lbl_set | model.rel_lbl_set)]      # all exited entities that appear in the sentence\n",
    "\n",
    "    # print(tokens)\n",
    "    word_seq = [' '.join(tokens[i:j+1]) for i in range(len(tokens)) for j in range(i, len(tokens))]\n",
    "    # print(word_seq)\n",
    "    matched_seq = [seq for seq in word_seq if seq in (model.ent_lbl_set | model.rel_lbl_set)]      # all exited entities that appear in the sentence\n",
    "    # print(matched_seq)\n",
    "    \n",
    "    \n",
    "    # extraction \n",
    "    ent_candidates = []\n",
    "    for seq in matched_seq:\n",
    "        if seq in model.ent_lbl_set:\n",
    "            ent_candidates.append(seq)\n",
    "        # detected relation\n",
    "        if seq in model.rel_lbl_set:\n",
    "            # which means there are two possible word for relations\n",
    "            if res['rel_lbl'] != None:\n",
    "                print(\"WARNIND: multiple possible relations detected...\")\n",
    "            res['rel'] = seq\n",
    "            res['rel_lbl'] = [seq]\n",
    "            res['rel_postfix'] = [model._get_rel_label(model.lbl2rel[seq])]\n",
    "            \n",
    "            \n",
    "    # extract entity from candidates\n",
    "    ent_candidates = sorted(ent_candidates, key = lambda x:len(x), reverse=True)\n",
    "    print(ent_candidates)\n",
    "    res['ent_lbl'] = ent_candidates[0]\n",
    "    \n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('utils/relation_ids.del', 'r') as ifile:\n",
    "    rel2id = {rdflib.term.URIRef(rel): int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\n",
    "id2rel = {v: k for k, v in rel2id.items()}\n",
    "with open('utils/entity_ids.del', 'r') as ifile:\n",
    "    ent2id = {rdflib.term.URIRef(ent): int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\n",
    "id2ent = {v: k for k, v in ent2id.items()}\n",
    "ent_emb = np.load('utils/entity_embeds.npy')\n",
    "rel_emb = np.load('utils/relation_embeds.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Star Wars: Episode VI â€“ Return of the Jedi', 'director', 'Return', 'Jedi', 'Who']\n",
      "['Star Wars: Episode VI â€“ Return of the Jedi', 'Star Wars Episode VI: Return of the Jedi', 'Star Wars: Return of the Jedi', 'Star Wars Episode II: Attack of the Clones', 'Star Wars: Episode III â€“ Revenge of the Sith', 'Star Wars: Episode II â€“ Attack of the Clones', 'Hidden Figures: The American Dream and the Untold Story of the Black Women Mathematicians Who Helped Win the Space Race', 'The Return of the Musketeers, or The Treasures of Cardinal Mazarin', 'Star Wars Episode IX: The Rise of Skywalker', \"Molly's Game: The True Story of the 26-Year-Old Woman Behind the Most Exclusive, High-Stakes Underground Poker Game in the World\"]\n",
      "{'ent_lbl': 'Star Wars: Episode VI â€“ Return of the Jedi', 'rel': 'director', 'rel_lbl': ['director', 'screenwriter', 'choreographer', 'KMRB film rating', 'described by source', 'art director', 'parent organization', 'INCAA film rating', 'headquarters location', 'operating system'], 'rel_postfix': ['P57', 'P58', 'P1809', 'P3818', 'P1343', 'P3174', 'P749', 'P3428', 'P159', 'P306']}\n",
      "PREFIX ddis: <http://ddis.ch/atai/>\n",
      "\n",
      "        PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "\n",
      "        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "\n",
      "        PREFIX schema: <http://schema.org/>\n",
      "\n",
      "        SELECT ?lbl WHERE {\n",
      "            ?sub rdfs:label \"Star Wars: Episode VI â€“ Return of the Jedi\"@en.\n",
      "\n",
      "            ?sub wdt:P57 ?obj.\n",
      "\n",
      "            ?obj rdfs:label ?lbl.\n",
      "\n",
      "        } LIMIT 1\n",
      "[['Richard Marquand']]\n",
      "PREFIX ddis: <http://ddis.ch/atai/>\n",
      "\n",
      "        PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "\n",
      "        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "\n",
      "        PREFIX schema: <http://schema.org/>\n",
      "\n",
      "        SELECT ?lbl WHERE {\n",
      "            ?sub rdfs:label \"Star Wars: Episode VI â€“ Return of the Jedi\"@en.\n",
      "\n",
      "            ?sub wdt:P58 ?obj.\n",
      "\n",
      "            ?obj rdfs:label ?lbl.\n",
      "\n",
      "        } LIMIT 1\n",
      "[['Lawrence Kasdan']]\n",
      "PREFIX ddis: <http://ddis.ch/atai/>\n",
      "\n",
      "        PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "\n",
      "        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "\n",
      "        PREFIX schema: <http://schema.org/>\n",
      "\n",
      "        SELECT ?lbl WHERE {\n",
      "            ?sub rdfs:label \"Star Wars: Episode VI â€“ Return of the Jedi\"@en.\n",
      "\n",
      "            ?sub wdt:P1809 ?obj.\n",
      "\n",
      "            ?obj rdfs:label ?lbl.\n",
      "\n",
      "        } LIMIT 1\n",
      "[]\n",
      "PREFIX ddis: <http://ddis.ch/atai/>\n",
      "\n",
      "        PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "\n",
      "        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "\n",
      "        PREFIX schema: <http://schema.org/>\n",
      "\n",
      "        SELECT ?lbl WHERE {\n",
      "            ?sub rdfs:label \"Star Wars: Episode VI â€“ Return of the Jedi\"@en.\n",
      "\n",
      "            ?sub wdt:P3818 ?obj.\n",
      "\n",
      "            ?obj rdfs:label ?lbl.\n",
      "\n",
      "        } LIMIT 1\n",
      "[]\n",
      "PREFIX ddis: <http://ddis.ch/atai/>\n",
      "\n",
      "        PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "\n",
      "        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "\n",
      "        PREFIX schema: <http://schema.org/>\n",
      "\n",
      "        SELECT ?lbl WHERE {\n",
      "            ?sub rdfs:label \"Star Wars: Episode VI â€“ Return of the Jedi\"@en.\n",
      "\n",
      "            ?sub wdt:P1343 ?obj.\n",
      "\n",
      "            ?obj rdfs:label ?lbl.\n",
      "\n",
      "        } LIMIT 1\n",
      "[]\n",
      "PREFIX ddis: <http://ddis.ch/atai/>\n",
      "\n",
      "        PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "\n",
      "        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "\n",
      "        PREFIX schema: <http://schema.org/>\n",
      "\n",
      "        SELECT ?lbl WHERE {\n",
      "            ?sub rdfs:label \"Star Wars: Episode VI â€“ Return of the Jedi\"@en.\n",
      "\n",
      "            ?sub wdt:P3174 ?obj.\n",
      "\n",
      "            ?obj rdfs:label ?lbl.\n",
      "\n",
      "        } LIMIT 1\n",
      "[]\n",
      "PREFIX ddis: <http://ddis.ch/atai/>\n",
      "\n",
      "        PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "\n",
      "        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "\n",
      "        PREFIX schema: <http://schema.org/>\n",
      "\n",
      "        SELECT ?lbl WHERE {\n",
      "            ?sub rdfs:label \"Star Wars: Episode VI â€“ Return of the Jedi\"@en.\n",
      "\n",
      "            ?sub wdt:P749 ?obj.\n",
      "\n",
      "            ?obj rdfs:label ?lbl.\n",
      "\n",
      "        } LIMIT 1\n",
      "[]\n",
      "PREFIX ddis: <http://ddis.ch/atai/>\n",
      "\n",
      "        PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "\n",
      "        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "\n",
      "        PREFIX schema: <http://schema.org/>\n",
      "\n",
      "        SELECT ?lbl WHERE {\n",
      "            ?sub rdfs:label \"Star Wars: Episode VI â€“ Return of the Jedi\"@en.\n",
      "\n",
      "            ?sub wdt:P3428 ?obj.\n",
      "\n",
      "            ?obj rdfs:label ?lbl.\n",
      "\n",
      "        } LIMIT 1\n",
      "[]\n",
      "PREFIX ddis: <http://ddis.ch/atai/>\n",
      "\n",
      "        PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "\n",
      "        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "\n",
      "        PREFIX schema: <http://schema.org/>\n",
      "\n",
      "        SELECT ?lbl WHERE {\n",
      "            ?sub rdfs:label \"Star Wars: Episode VI â€“ Return of the Jedi\"@en.\n",
      "\n",
      "            ?sub wdt:P159 ?obj.\n",
      "\n",
      "            ?obj rdfs:label ?lbl.\n",
      "\n",
      "        } LIMIT 1\n",
      "[]\n",
      "PREFIX ddis: <http://ddis.ch/atai/>\n",
      "\n",
      "        PREFIX wd: <http://www.wikidata.org/entity/>\n",
      "\n",
      "        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
      "\n",
      "        PREFIX schema: <http://schema.org/>\n",
      "\n",
      "        SELECT ?lbl WHERE {\n",
      "            ?sub rdfs:label \"Star Wars: Episode VI â€“ Return of the Jedi\"@en.\n",
      "\n",
      "            ?sub wdt:P306 ?obj.\n",
      "\n",
      "            ?obj rdfs:label ?lbl.\n",
      "\n",
      "        } LIMIT 1\n",
      "[]\n",
      "film\n"
     ]
    }
   ],
   "source": [
    "# testing cases\n",
    "# user_query = \"Who is the screenwriter of The Masked Gang: Cyprus? \"\n",
    "user_query = 'Who is the director of Star Wars: Episode VI - Return of the Jedi? '\n",
    "# user_query = 'When was \"The Godfather\" release?' \n",
    "# user_query = 'What is the publication date of \"The Godfather\" ? '\n",
    "\n",
    "ruler_attemp = _ruler_based(KG_handler, user_query)\n",
    "# print(ruler_attemp)\n",
    "\n",
    "# calling backup extraction strategy\n",
    "if None in ruler_attemp.values():\n",
    "    similarity_attemp = _similarity_based(KG_handler, user_query)\n",
    "    for k,v in ruler_attemp.items():\n",
    "        if v == None:\n",
    "            ruler_attemp[k] = similarity_attemp[k]\n",
    "            \n",
    "similarity_attemp = _similarity_based(KG_handler, user_query)\n",
    "print(similarity_attemp)\n",
    "# print(similarity_attemp)\n",
    "# print(ruler_attemp)\n",
    "ruler_attemp = similarity_attemp\n",
    "# grab result from graph\n",
    "for i in range(len(ruler_attemp['rel_postfix'])):\n",
    "    movie_name = ruler_attemp['ent_lbl']\n",
    "    target_label = ruler_attemp['rel_postfix'][i]\n",
    "    target_name = ruler_attemp['rel_lbl'][i]\n",
    "    if \"date\" in target_name:\n",
    "        query = f'''PREFIX ddis: <http://ddis.ch/atai/>\n",
    "\n",
    "        PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "\n",
    "        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "\n",
    "        PREFIX schema: <http://schema.org/>\n",
    "\n",
    "        SELECT ?date WHERE {{\n",
    "            ?movie rdfs:label \"{movie_name}\"@en.\n",
    "\n",
    "            ?movie wdt:{target_label} ?date\n",
    "\n",
    "        }} LIMIT 1'''\n",
    "    else:\n",
    "        query = f'''PREFIX ddis: <http://ddis.ch/atai/>\n",
    "\n",
    "        PREFIX wd: <http://www.wikidata.org/entity/>\n",
    "\n",
    "        PREFIX wdt: <http://www.wikidata.org/prop/direct/>\n",
    "\n",
    "        PREFIX schema: <http://schema.org/>\n",
    "\n",
    "        SELECT ?lbl WHERE {{\n",
    "            ?sub rdfs:label \"{movie_name}\"@en.\n",
    "\n",
    "            ?sub wdt:{target_label} ?obj.\n",
    "\n",
    "            ?obj rdfs:label ?lbl.\n",
    "\n",
    "        }} LIMIT 1'''\n",
    "    query  = query.strip()\n",
    "    print(query)\n",
    "    res = []\n",
    "    for row in KG_handler.graph.query(query):\n",
    "        res.append([str(i) for i in row]) \n",
    "    print(res)\n",
    "    # stop if any result grabed \n",
    "    if False:\n",
    "        break\n",
    "    \n",
    "# if no any query result match, using embeddings\n",
    "ent_id = ent2id[KG_handler.lbl2ent[ruler_attemp['ent_lbl']]]\n",
    "rel_id = rel2id[KG_handler.lbl2rel[ruler_attemp['rel_lbl'][0]]]\n",
    "head = ent_emb[ent_id]\n",
    "pred = rel_emb[rel_id]\n",
    "\n",
    "lhs = (head - pred).reshape((1,-1))\n",
    "\n",
    "dist = pairwise_distances(lhs, ent_emb).flatten()\n",
    "most_likely_idx = dist.argsort()[0]\n",
    "closest_ent = KG_handler.ent2lbl[id2ent[most_likely_idx]]\n",
    "\n",
    "print(closest_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "operator\n",
      "different from\n",
      "Minkultury film ID\n",
      "designed by\n",
      "shares border with\n",
      "director of photography\n",
      "narrator\n",
      "publisher\n",
      "replaces\n",
      "director\n",
      "director / manager\n",
      "published in\n",
      "takes place in fictional universe\n",
      "cast member\n",
      "list of works\n",
      "time period\n",
      "interested in\n",
      "main subject\n",
      "allegiance\n",
      "lifestyle\n",
      "continent\n",
      "original film format\n",
      "author\n",
      "stepparent\n",
      "capital of\n",
      "performer\n",
      "BBFC rating\n",
      "operating area\n",
      "contributor to the creative work or subject\n",
      "YouTube video ID\n",
      "OFLC classification\n",
      "choreographer\n",
      "followed by\n",
      "superhuman feature or ability\n",
      "voice actor\n",
      "platform\n",
      "occupant\n",
      "mother\n",
      "assessment\n",
      "ethnic group\n",
      "relative\n",
      "medical condition\n",
      "intended public\n",
      "place of birth\n",
      "after a work by\n",
      "IMDb ID\n",
      "founded by\n",
      "work location\n",
      "field of work\n",
      "based on\n",
      "inspired by\n",
      "Disney A to Z ID\n",
      "iTunes movie ID\n",
      "participant in\n",
      "religion\n",
      "collection\n",
      "copyright holder\n",
      "sports discipline competed in\n",
      "Filmiroda rating\n",
      "Writers Guild of America project ID\n",
      "has works in the collection\n",
      "The Criterion Collection film ID\n",
      "given name\n",
      "has quality\n",
      "country\n",
      "copyright status\n",
      "production designer\n",
      "archives at\n",
      "Microsoft Store product ID\n",
      "creator\n",
      "playDB play ID\n",
      "place of burial\n",
      "replaced by\n",
      "cites work\n",
      "presenter\n",
      "character designer\n",
      "language of work or name\n",
      "IMDA rating\n",
      "occupation\n",
      "ClassInd rating\n",
      "nominated for\n",
      "diplomatic relation\n",
      "animator\n",
      "sport\n",
      "Vudu video ID\n",
      "influenced by\n",
      "has cause\n",
      "basin country\n",
      "affiliation\n",
      "CNC film rating (France)\n",
      "twinned administrative body\n",
      "characters\n",
      "KMRB film rating\n",
      "box office\n",
      "production company\n",
      "RCQ classification\n",
      "follows\n",
      "JMK film rating\n",
      "from narrative universe\n",
      "represented by\n",
      "input method\n",
      "place of publication\n",
      "educated at\n",
      "significant event\n",
      "ICAA film catalogue ID\n",
      "military branch\n",
      "Kijkwijzer rating\n",
      "part of the series\n",
      "set during recurring event\n",
      "member of the crew of\n",
      "cause of death\n",
      "operating system\n",
      "copyright license\n",
      "references work, tradition or theory\n",
      "subclass of\n",
      "executive producer\n",
      "Swedish Film Database film ID\n",
      "convicted of\n",
      "official residence\n",
      "narrative location\n",
      "film crew member\n",
      "RTC film rating\n",
      "located in present-day administrative territorial entity\n",
      "developer\n",
      "part of\n",
      "partner in business or sport\n",
      "head of government\n",
      "FPB rating\n",
      "scenographer\n",
      "significant person\n",
      "publication date\n",
      "present in work\n",
      "IGAC rating\n",
      "lowest point\n",
      "located on terrain feature\n",
      "BFI National Archive work ID\n",
      "TV.com ID\n",
      "Rotten Tomatoes ID\n",
      "head of state\n",
      "Disney+ movie ID\n",
      "native language\n",
      "public holiday\n",
      "award received\n",
      "set in period\n",
      "genre\n",
      "place of death\n",
      "sibling\n",
      "health specialty\n",
      "make-up artist\n",
      "Hulu movie ID\n",
      "historic county\n",
      "language used\n",
      "member of sports team\n",
      "indigenous to\n",
      "screenwriter\n",
      "film editor\n",
      "narrative motif\n",
      "parent organization\n",
      "spouse\n",
      "opposite of\n",
      "use\n",
      "winner\n",
      "industry\n",
      "manner of death\n",
      "fictional universe described in\n",
      "sound designer\n",
      "Australian Classification\n",
      "MPAA film rating\n",
      "official language\n",
      "partially coincident with\n",
      "KAVI rating\n",
      "fabrication method\n",
      "movement\n",
      "color\n",
      "Apple TV movie ID\n",
      "ISAN\n",
      "art director\n",
      "field of this occupation\n",
      "FSK film rating\n",
      "season\n",
      "MTRCB rating\n",
      "noble title\n",
      "image\n",
      "conflict\n",
      "presented in\n",
      "derivative work\n",
      "CNC film rating (Romania)\n",
      "Hong Kong film rating\n",
      "home world\n",
      "country of citizenship\n",
      "political ideology\n",
      "has edition or translation\n",
      "plot expanded in\n",
      "named after\n",
      "quotes work\n",
      "sex or gender\n",
      "employer\n",
      "EIRIN film rating\n",
      "media franchise\n",
      "owned by\n",
      "FandangoNow ID\n",
      "Comic Vine ID\n",
      "described by source\n",
      "located in or next to body of water\n",
      "instance of\n",
      "original language of film or TV show\n",
      "copyright representative\n",
      "distributed by\n",
      "filming location\n",
      "unmarried partner\n",
      "located on street\n",
      "IFCO rating\n",
      "Libreflix ID\n",
      "has effect\n",
      "ICAA rating\n",
      "student of\n",
      "location\n",
      "owner of\n",
      "uses\n",
      "headquarters location\n",
      "sidekick of\n",
      "practiced by\n",
      "costume designer\n",
      "distribution format\n",
      "original broadcaster\n",
      "writing language\n",
      "place served by transport hub\n",
      "participant\n",
      "member of\n",
      "has pet\n",
      "edition or translation of\n",
      "contains administrative territorial entity\n",
      "Filmportal ID\n",
      "Filmweb.pl ID\n",
      "ACMI web ID\n",
      "HBO Max ID\n",
      "languages spoken, written or signed\n",
      "depicts\n",
      "RARS rating\n",
      "musical conductor\n",
      "ÄŒSFD film ID\n",
      "applies to jurisdiction\n",
      "product or material produced\n",
      "business model\n",
      "CÃ©sar Award film ID\n",
      "facet of\n",
      "dedicated to\n",
      "Netflix ID\n",
      "MedierÃ¥det rating\n",
      "set in environment\n",
      "broadcast by\n",
      "depicted by\n",
      "first appearance\n",
      "capital\n",
      "storyboard artist\n",
      "said to be the same as\n",
      "Metacritic ID\n",
      "killed by\n",
      "has part\n",
      "enemy of\n",
      "TCM Movie Database film ID\n",
      "student\n",
      "father\n",
      "aspect ratio\n",
      "form of creative work\n",
      "BAMID film rating\n",
      "conferred by\n",
      "child\n",
      "sexual orientation\n",
      "location of formation\n",
      "INCAA film rating\n",
      "crew member(s)\n",
      "place of detention\n",
      "located in the administrative territorial entity\n",
      "permanent resident of\n",
      "residence\n",
      "ancestral home\n",
      "notable work\n",
      "Fandango film ID\n",
      "country for sport\n",
      "NMHH film rating\n",
      "country of origin\n"
     ]
    }
   ],
   "source": [
    "lm = KG_handler.LM\n",
    "for i in KG_handler.rel_lbl_set:\n",
    "    # print(i)\n",
    "    KG_handler.rel_lbl2vec[i] = lm.get_sentence_vector(i)\n",
    "# lm.get_sentence_vector('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speakeasy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
